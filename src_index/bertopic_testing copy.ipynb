{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pdoub\\Desktop\\python_projects\\junk-drawer\\.venv\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\pdoub\\Desktop\\python_projects\\junk-drawer\\.venv\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\pdoub\\Desktop\\python_projects\\junk-drawer\\.venv\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\pdoub\\Desktop\\python_projects\\junk-drawer\\.venv\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance\n",
    "import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"reddit_legal_cluster_test_results.parquet\")\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>full_link</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>text_label</th>\n",
       "      <th>flair_label</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>token_count</th>\n",
       "      <th>llm_title</th>\n",
       "      <th>State</th>\n",
       "      <th>kmeans_label</th>\n",
       "      <th>topic_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4994</td>\n",
       "      <td>1475721018</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n",
       "      <td>5639t2</td>\n",
       "      <td>I will preface this by admitting I'm an idiot ...</td>\n",
       "      <td>(CO) First Ticket ever, Careless Driving</td>\n",
       "      <td>driving</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.004459863429254688, 0.003479491143272819, 0...</td>\n",
       "      <td>209</td>\n",
       "      <td>\"Legal options for a teenager facing a Careles...</td>\n",
       "      <td>NJ</td>\n",
       "      <td>10</td>\n",
       "      <td>Legal Topics in Traffic Violations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>8627</td>\n",
       "      <td>1589322350</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n",
       "      <td>gilww3</td>\n",
       "      <td>Hey folks;\\n\\nI'm currently designing a video ...</td>\n",
       "      <td>[OH] Advice on safe contracts for freelancers/...</td>\n",
       "      <td>contract</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.010599094232140971, -0.01940659953784975, -...</td>\n",
       "      <td>191</td>\n",
       "      <td>\"Protecting Intellectual Property and Ensuring...</td>\n",
       "      <td>OR</td>\n",
       "      <td>6</td>\n",
       "      <td>Compilation of Legal Topics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  created_utc                                          full_link  \\\n",
       "4998   4994   1475721018  https://www.reddit.com/r/legaladvice/comments/...   \n",
       "4999   8627   1589322350  https://www.reddit.com/r/legaladvice/comments/...   \n",
       "\n",
       "          id                                               body  \\\n",
       "4998  5639t2  I will preface this by admitting I'm an idiot ...   \n",
       "4999  gilww3  Hey folks;\\n\\nI'm currently designing a video ...   \n",
       "\n",
       "                                                  title text_label  \\\n",
       "4998           (CO) First Ticket ever, Careless Driving    driving   \n",
       "4999  [OH] Advice on safe contracts for freelancers/...   contract   \n",
       "\n",
       "      flair_label                                         embeddings  \\\n",
       "4998            4  [0.004459863429254688, 0.003479491143272819, 0...   \n",
       "4999            1  [0.010599094232140971, -0.01940659953784975, -...   \n",
       "\n",
       "      token_count                                          llm_title State  \\\n",
       "4998          209  \"Legal options for a teenager facing a Careles...    NJ   \n",
       "4999          191  \"Protecting Intellectual Property and Ensuring...    OR   \n",
       "\n",
       "      kmeans_label                         topic_title  \n",
       "4998            10  Legal Topics in Traffic Violations  \n",
       "4999             6         Compilation of Legal Topics  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "class TopicEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_col, threshold=0.1):\n",
    "        self.text_col = text_col\n",
    "        self.threshold = threshold\n",
    "        self.vectorizer_model = CountVectorizer(stop_words = 'english')\n",
    "        self.topic_model = BERTopic(\n",
    "            vectorizer_model = self.vectorizer_model,\n",
    "            nr_topics = 'auto',\n",
    "            calculate_probabilities=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        docs = list(X[self.text_col])\n",
    "        self.topic_model.fit(docs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        docs = list(X[self.text_col])\n",
    "        # X = X.join(self.topic_model.get_document_info(docs))\n",
    "        topic_distr, _ = self.topic_model.approximate_distribution(docs, window = 4, calculate_tokens=True)\n",
    "        X['multiple_topics'] = list(map(\n",
    "            lambda doc_topic_distr: list(map(\n",
    "                lambda y: y[0], filter(lambda x: x[1] >= self.threshold, \n",
    "                                    (enumerate(doc_topic_distr)))\n",
    "            )), topic_distr\n",
    "        ))\n",
    "        X = self.one_hot_encode_topics(X, 'multiple_topics')\n",
    "        return X\n",
    "\n",
    "    def one_hot_encode_topics(self, df, column_name):\n",
    "        df[column_name] = df[column_name].astype('string')\n",
    "        one_hot_list = []\n",
    "        for idx, row in df.iterrows():\n",
    "            topics = eval(row[column_name])\n",
    "            one_hot_row = {}\n",
    "            for topic in topics:\n",
    "                one_hot_row[f'topic_{topic}'] = 1\n",
    "            one_hot_list.append(one_hot_row)\n",
    "        one_hot_df = pd.DataFrame(one_hot_list)\n",
    "        one_hot_df.fillna(0, inplace=True)\n",
    "        one_hot_df = one_hot_df.astype(int)\n",
    "\n",
    "        # Sort the columns\n",
    "        one_hot_df = one_hot_df.sort_index(axis=1)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df = pd.concat([df, one_hot_df], axis=1)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from fastembed.embedding import FlagEmbedding as Embedding\n",
    "import umap\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingUmapTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_column, model_name=\"BAAI/bge-base-en-v1.5\", max_length=512, n_neighbors=10, metric='cosine', min_dist=0.0, n_components=5):\n",
    "        self.text_column = text_column\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components = n_components\n",
    "        self.umap_model = umap.UMAP(angular_rp_forest=True, n_neighbors=n_neighbors, metric=metric, min_dist=min_dist, n_components=n_components)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.embedding_model = Embedding(model_name=self.model_name, max_length=self.max_length)\n",
    "        documents = (\"passage: \" + text for text in X[self.text_column])\n",
    "        embeddings = self.embedding_model.embed(documents)\n",
    "        self.umap_model.fit(np.vstack(embeddings))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.embedding_model = Embedding(model_name=self.model_name, max_length=self.max_length)\n",
    "        documents = (\"passage: \" + text for text in X[self.text_column])\n",
    "        embeddings = self.embedding_model.embed(documents)\n",
    "        reduced_embeddings = self.umap_model.transform(np.vstack(embeddings))\n",
    "        for i in range(reduced_embeddings.shape[1]):\n",
    "            X[f'umap_{i}'] = reduced_embeddings[:, i]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from collections import Counter\n",
    "\n",
    "class KeywordVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_features=None, text_col_name=None):\n",
    "        self.max_features = max_features\n",
    "        self.text_col_name = text_col_name\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.kw_model = KeyBERT(model=self.nlp)\n",
    "        self.vectorizer = KeyphraseCountVectorizer(\n",
    "            spacy_pipeline=self.nlp,\n",
    "            )\n",
    "        self.vectorizer_model = None\n",
    "        self.vocabulary = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        docs = X[self.text_col_name]\n",
    "        keywords = self.kw_model.extract_keywords(\n",
    "            docs,\n",
    "            vectorizer=self.vectorizer,\n",
    "            top_n=10,\n",
    "            use_mmr=True)\n",
    "        keyword_counts = Counter(k[0] for keyword in keywords for k in keyword)\n",
    "        self.vocabulary = [k for k, _ in keyword_counts.most_common(self.max_features)]\n",
    "        self.vectorizer_model = TfidfVectorizer(vocabulary=self.vocabulary)\n",
    "        self.vectorizer_model.fit(docs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        vectors = self.vectorizer_model.transform(df[self.text_col_name])\n",
    "        new_columns = pd.DataFrame(vectors.toarray(), columns=['count_vec_' + str(i) for i in range(vectors.shape[1])])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df = pd.concat([df, new_columns], axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import coreferee\n",
    "from typing import Counter\n",
    "from gensim.models import FastText\n",
    "import spacy.cli\n",
    "\n",
    "def check_and_install_spacy_models(models):\n",
    "    for model in models:\n",
    "        try:\n",
    "            # Try to load the model\n",
    "            spacy.load(model)\n",
    "        except OSError:\n",
    "            # If model is not installed, download and install it\n",
    "            print(f\"{model} not found. Downloading and installing...\")\n",
    "            spacy.cli.download(model)\n",
    "# List of models to check and install\n",
    "models = [\"en_core_web_md\", \"en_core_web_lg\"]\n",
    "\n",
    "check_and_install_spacy_models(models)\n",
    "\n",
    "class TripleExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, text_column, risk_keywords, n_top_triples=20, corpus=None):\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.nlp1 = spacy.load('en_core_web_lg')\n",
    "        self.nlp1.add_pipe('coreferee')\n",
    "        self.nlp1 = spacy.load(\"en_core_web_lg\")\n",
    "        if \"coreferee\" not in self.nlp1.pipe_names:\n",
    "            self.nlp1.add_pipe('coreferee')\n",
    "        self.nlp2 = spacy.load(\"en_core_web_md\")\n",
    "        self.text_column = text_column\n",
    "        self.risk_keywords = risk_keywords\n",
    "        self.n_top_triples = n_top_triples\n",
    "        self.corpus = corpus\n",
    "        self.top_triples = None\n",
    "        self.risk_triples = None\n",
    "        \n",
    "        # If a corpus is provided, train Word2Vec on it\n",
    "        if corpus:\n",
    "            sentences = [text.split() for text in corpus]\n",
    "            self.fasttext_model = FastText(sentences, window=5, min_count=1, workers=4)    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Extract all triplets from the training data\n",
    "        all_triples = self._extract_all_triples(X[self.text_column])\n",
    "        \n",
    "        # Find the most common triplets\n",
    "        triple_counter = Counter(all_triples)\n",
    "        self.top_triples = [triple for triple, count in triple_counter.most_common(self.n_top_triples)]\n",
    "        \n",
    "        # Find the risk-related triplets\n",
    "        self.risk_triples = [triple for triple in all_triples if any(self.fasttext_model.wv.similarity(keyword, word) > 0.8 for word in triple for keyword in self.risk_keywords)]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Initialize a dictionary to store the counts of each risk keyword\n",
    "        risk_keyword_counts = {keyword: [] for keyword in self.risk_keywords}\n",
    "        top_feature_vectors = []\n",
    "        for texts in tqdm.tqdm(X[self.text_column]):\n",
    "            text_triples = self._extract_triples_from_text(texts)\n",
    "            top_features = [int(triple in text_triples) for triple in self.top_triples]\n",
    "            top_feature_vectors.append(top_features)\n",
    "            # Count the occurrences of each risk keyword in the text triples\n",
    "            for keyword in self.risk_keywords:\n",
    "                risk_keyword_counts[keyword].append(sum(keyword in triple for triple in text_triples))\n",
    "        # Convert the dictionary to a DataFrame\n",
    "        risk_keyword_df = pd.DataFrame(risk_keyword_counts)\n",
    "        \n",
    "        # Convert the lists of feature vectors to DataFrames\n",
    "        top_feature_df = pd.DataFrame(top_feature_vectors, columns=[\"top_triples_\" + str(triple) for triple in self.top_triples])\n",
    "        \n",
    "        # Clean the column names in the top_feature_df\n",
    "        top_feature_df.columns = top_feature_df.columns.str.lower().str.replace(' ', '_').str.replace('[^a-z0-9_]', '').str.replace(\"('\", \"\").str.replace(\"',_'\", \"_\").str.replace(\"')\", \"\")\n",
    "        \n",
    "        # Concatenate the new features to the original DataFrame\n",
    "        X_extended = pd.concat([X.reset_index(drop=True), top_feature_df, risk_keyword_df], axis=1)\n",
    "        \n",
    "        return X_extended\n",
    "\n",
    "    def _extract_all_triples(self, X):\n",
    "        # This method extracts all triplets from a list of texts\n",
    "        all_triples = []\n",
    "        for texts in X:\n",
    "            text_triples = self._extract_triples_from_text(texts)\n",
    "            all_triples.extend(text_triples)\n",
    "        return all_triples\n",
    "\n",
    "    def _extract_triples_from_text(self, text):\n",
    "        triples = []\n",
    "\n",
    "        # Coreference resolution\n",
    "        text = self.coref_resolve(text)\n",
    "\n",
    "        # Split compound sentences into simple sentences\n",
    "        simple_sentences = self.compound_to_simple(text)\n",
    "\n",
    "        for sentence in simple_sentences:\n",
    "            doc = self.nlp(sentence)\n",
    "            svo_triples = list(textacy.extract.subject_verb_object_triples(doc))\n",
    "            for triple in svo_triples:\n",
    "                subj = \"_\".join(map(str, triple.subject))\n",
    "                obj = \"_\".join(map(str, triple.object))\n",
    "                verb = \"_\".join(map(str, triple.verb))\n",
    "                triples.append((subj, verb, obj))\n",
    "        return triples\n",
    "\n",
    "    def coref_resolve(self, text):\n",
    "        doc1 = self.nlp1(text)\n",
    "        tok_list = list(token.text for token in doc1)\n",
    "        c = 0\n",
    "        for chain in doc1._.coref_chains:\n",
    "            for mention in chain:\n",
    "                res1 = [doc1._.coref_chains.resolve(doc1[i]) for i in mention]\n",
    "                res = [r for r in res1 if r is not None]\n",
    "                if len(res) != 0:\n",
    "                    if len(res[0]) == 1:\n",
    "                        tok_list[mention[0] + c] = str(res[0][0])\n",
    "                    elif len(res[0]) > 1:\n",
    "                        tok_list[mention[0] + c] = str(res[0][0])\n",
    "                        for j in range(1, len(res[0])):\n",
    "                            tok_list.insert(mention[0] + c + j, str(res[0][j]))\n",
    "                            c = c + 1\n",
    "        textres = \" \".join(tok_list)\n",
    "        return textres\n",
    "\n",
    "    def compound_to_simple(self, sentence):\n",
    "        doc = self.nlp2(sentence)\n",
    "        root_token = None\n",
    "        for token in doc:\n",
    "            if (token.dep_ == \"ROOT\"):\n",
    "                root_token = token\n",
    "        other_verbs = []\n",
    "        for token in doc:\n",
    "            ancestors = list(token.ancestors)\n",
    "            if (token.pos_ == \"VERB\" and len(ancestors) < 3 and token != root_token):\n",
    "                other_verbs.append(token)\n",
    "        token_spans = []\n",
    "        all_verbs = [root_token] + other_verbs\n",
    "        for other_verb in all_verbs:\n",
    "            first_token_index = len(doc)\n",
    "            last_token_index = 0\n",
    "            this_verb_children = list(other_verb.children)\n",
    "            for child in this_verb_children:\n",
    "                if (child not in all_verbs):\n",
    "                    if (child.i < first_token_index):\n",
    "                        first_token_index = child.i\n",
    "                    if (child.i > last_token_index):\n",
    "                        last_token_index = child.i\n",
    "            token_spans.append((first_token_index, last_token_index))\n",
    "        sentence_clauses = []\n",
    "        for token_span in token_spans:\n",
    "            start = token_span[0]\n",
    "            end = token_span[1]\n",
    "            if (start < end):\n",
    "                clause = doc[start:end]\n",
    "                sentence_clauses.append(clause)\n",
    "        sentence_clauses = sorted(sentence_clauses, key=lambda tup: tup[0])\n",
    "        clauses_text = [clause.text for clause in sentence_clauses]\n",
    "        return clauses_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_keywords = [\"liability\", \"insurance\", \"should_do\"]\n",
    "\n",
    "corpus_ = df['body'].tolist()\n",
    "corpus = [d for d in corpus_]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('topic_encoder', TopicEncoder(text_col=\"body\")),\n",
    "    ('kw_count_vectorizer', KeywordVectorizer(max_features=50, text_col_name=\"body\")),\n",
    "    ('triplet_extractor', TripleExtractor(\n",
    "        'body', \n",
    "        risk_keywords=risk_keywords, \n",
    "        n_top_triples=10, \n",
    "        corpus=corpus)),\n",
    "    ('umap_embedder', EmbeddingUmapTransformer(text_column=\"body\"))\n",
    "],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 4) Processing topic_encoder, total=   5.8s\n",
      "[Pipeline]  (step 2 of 4) Processing kw_count_vectorizer, total=  11.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] . (step 3 of 4) Processing triplet_extractor, total=  50.2s\n",
      "[Pipeline] ..... (step 4 of 4) Processing umap_embedder, total=  32.3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;topic_encoder&#x27;, TopicEncoder(text_col=&#x27;body&#x27;)),\n",
       "                (&#x27;kw_count_vectorizer&#x27;,\n",
       "                 KeywordVectorizer(max_features=50, text_col_name=&#x27;body&#x27;)),\n",
       "                (&#x27;triplet_extractor&#x27;,\n",
       "                 TripleExtractor(corpus=[&#x27;I applied for a job and after two &#x27;\n",
       "                                         &#x27;interviews I was given for and &#x27;\n",
       "                                         &#x27;agreed to an offer letter, had a &#x27;\n",
       "                                         &#x27;start date.  The only remaining &#x27;\n",
       "                                         &#x27;variable was a drug test, which I &#x27;\n",
       "                                         &#x27;prepared myself for by b...\n",
       "                                         &#x27;no reason. You’d think something &#x27;\n",
       "                                         &#x27;representing something for LEGAL &#x27;\n",
       "                                         &#x27;ADVICE would help someone looking &#x27;\n",
       "                                         &#x27;for LEGAL ADVICE but apparently &#x27;\n",
       "                                         &#x27;not \\n&#x27;\n",
       "                                         &#x27;\\n&#x27;\n",
       "                                         &#x27;I want to know this answer I don’t &#x27;\n",
       "                                         &#x27;see what’s wrong with that. I &#x27;\n",
       "                                         &#x27;GENUINELY don’t&#x27;, ...],\n",
       "                                 n_top_triples=10,\n",
       "                                 risk_keywords=[&#x27;liability&#x27;, &#x27;insurance&#x27;,\n",
       "                                                &#x27;should_do&#x27;],\n",
       "                                 text_column=&#x27;body&#x27;)),\n",
       "                (&#x27;umap_embedder&#x27;,\n",
       "                 EmbeddingUmapTransformer(text_column=&#x27;body&#x27;))],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;topic_encoder&#x27;, TopicEncoder(text_col=&#x27;body&#x27;)),\n",
       "                (&#x27;kw_count_vectorizer&#x27;,\n",
       "                 KeywordVectorizer(max_features=50, text_col_name=&#x27;body&#x27;)),\n",
       "                (&#x27;triplet_extractor&#x27;,\n",
       "                 TripleExtractor(corpus=[&#x27;I applied for a job and after two &#x27;\n",
       "                                         &#x27;interviews I was given for and &#x27;\n",
       "                                         &#x27;agreed to an offer letter, had a &#x27;\n",
       "                                         &#x27;start date.  The only remaining &#x27;\n",
       "                                         &#x27;variable was a drug test, which I &#x27;\n",
       "                                         &#x27;prepared myself for by b...\n",
       "                                         &#x27;no reason. You’d think something &#x27;\n",
       "                                         &#x27;representing something for LEGAL &#x27;\n",
       "                                         &#x27;ADVICE would help someone looking &#x27;\n",
       "                                         &#x27;for LEGAL ADVICE but apparently &#x27;\n",
       "                                         &#x27;not \\n&#x27;\n",
       "                                         &#x27;\\n&#x27;\n",
       "                                         &#x27;I want to know this answer I don’t &#x27;\n",
       "                                         &#x27;see what’s wrong with that. I &#x27;\n",
       "                                         &#x27;GENUINELY don’t&#x27;, ...],\n",
       "                                 n_top_triples=10,\n",
       "                                 risk_keywords=[&#x27;liability&#x27;, &#x27;insurance&#x27;,\n",
       "                                                &#x27;should_do&#x27;],\n",
       "                                 text_column=&#x27;body&#x27;)),\n",
       "                (&#x27;umap_embedder&#x27;,\n",
       "                 EmbeddingUmapTransformer(text_column=&#x27;body&#x27;))],\n",
       "         verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TopicEncoder</label><div class=\"sk-toggleable__content\"><pre>TopicEncoder(text_col=&#x27;body&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KeywordVectorizer</label><div class=\"sk-toggleable__content\"><pre>KeywordVectorizer(max_features=50, text_col_name=&#x27;body&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TripleExtractor</label><div class=\"sk-toggleable__content\"><pre>TripleExtractor(corpus=[&#x27;I applied for a job and after two interviews I was &#x27;\n",
       "                        &#x27;given for and agreed to an offer letter, had a start &#x27;\n",
       "                        &#x27;date.  The only remaining variable was a drug test, &#x27;\n",
       "                        &#x27;which I prepared myself for by bringing all my &#x27;\n",
       "                        &#x27;prescriptions with me.  \\n&#x27;\n",
       "                        &#x27;\\n&#x27;\n",
       "                        &#x27;I take Adderall, my doctor prescribes me for 3 pills &#x27;\n",
       "                        &#x27;a day, insurance pays for only two a day so that’s &#x27;\n",
       "                        &#x27;what I get.  2 pills a day for 30 days.  \\n&#x27;\n",
       "                        &#x27;\\n&#x27;\n",
       "                        &#x27;How...\n",
       "                        &#x27;I’ve posted this multiple times and the moderator &#x27;\n",
       "                        &#x27;keeps removing me for no reason. You’d think &#x27;\n",
       "                        &#x27;something representing something for LEGAL ADVICE &#x27;\n",
       "                        &#x27;would help someone looking for LEGAL ADVICE but &#x27;\n",
       "                        &#x27;apparently not \\n&#x27;\n",
       "                        &#x27;\\n&#x27;\n",
       "                        &#x27;I want to know this answer I don’t see what’s wrong &#x27;\n",
       "                        &#x27;with that. I GENUINELY don’t&#x27;, ...],\n",
       "                n_top_triples=10,\n",
       "                risk_keywords=[&#x27;liability&#x27;, &#x27;insurance&#x27;, &#x27;should_do&#x27;],\n",
       "                text_column=&#x27;body&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EmbeddingUmapTransformer</label><div class=\"sk-toggleable__content\"><pre>EmbeddingUmapTransformer(text_column=&#x27;body&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('topic_encoder', TopicEncoder(text_col='body')),\n",
       "                ('kw_count_vectorizer',\n",
       "                 KeywordVectorizer(max_features=50, text_col_name='body')),\n",
       "                ('triplet_extractor',\n",
       "                 TripleExtractor(corpus=['I applied for a job and after two '\n",
       "                                         'interviews I was given for and '\n",
       "                                         'agreed to an offer letter, had a '\n",
       "                                         'start date.  The only remaining '\n",
       "                                         'variable was a drug test, which I '\n",
       "                                         'prepared myself for by b...\n",
       "                                         'no reason. You’d think something '\n",
       "                                         'representing something for LEGAL '\n",
       "                                         'ADVICE would help someone looking '\n",
       "                                         'for LEGAL ADVICE but apparently '\n",
       "                                         'not \\n'\n",
       "                                         '\\n'\n",
       "                                         'I want to know this answer I don’t '\n",
       "                                         'see what’s wrong with that. I '\n",
       "                                         'GENUINELY don’t', ...],\n",
       "                                 n_top_triples=10,\n",
       "                                 risk_keywords=['liability', 'insurance',\n",
       "                                                'should_do'],\n",
       "                                 text_column='body')),\n",
       "                ('umap_embedder',\n",
       "                 EmbeddingUmapTransformer(text_column='body'))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.04it/s]\n"
     ]
    }
   ],
   "source": [
    "test_res = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>full_link</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>text_label</th>\n",
       "      <th>flair_label</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>token_count</th>\n",
       "      <th>llm_title</th>\n",
       "      <th>State</th>\n",
       "      <th>kmeans_label</th>\n",
       "      <th>topic_title</th>\n",
       "      <th>multiple_topics</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>count_vec_0</th>\n",
       "      <th>count_vec_1</th>\n",
       "      <th>count_vec_2</th>\n",
       "      <th>count_vec_3</th>\n",
       "      <th>count_vec_4</th>\n",
       "      <th>count_vec_5</th>\n",
       "      <th>count_vec_6</th>\n",
       "      <th>count_vec_7</th>\n",
       "      <th>count_vec_8</th>\n",
       "      <th>count_vec_9</th>\n",
       "      <th>count_vec_10</th>\n",
       "      <th>count_vec_11</th>\n",
       "      <th>count_vec_12</th>\n",
       "      <th>count_vec_13</th>\n",
       "      <th>count_vec_14</th>\n",
       "      <th>count_vec_15</th>\n",
       "      <th>count_vec_16</th>\n",
       "      <th>count_vec_17</th>\n",
       "      <th>count_vec_18</th>\n",
       "      <th>count_vec_19</th>\n",
       "      <th>count_vec_20</th>\n",
       "      <th>count_vec_21</th>\n",
       "      <th>count_vec_22</th>\n",
       "      <th>count_vec_23</th>\n",
       "      <th>count_vec_24</th>\n",
       "      <th>count_vec_25</th>\n",
       "      <th>count_vec_26</th>\n",
       "      <th>count_vec_27</th>\n",
       "      <th>count_vec_28</th>\n",
       "      <th>count_vec_29</th>\n",
       "      <th>count_vec_30</th>\n",
       "      <th>count_vec_31</th>\n",
       "      <th>count_vec_32</th>\n",
       "      <th>count_vec_33</th>\n",
       "      <th>count_vec_34</th>\n",
       "      <th>count_vec_35</th>\n",
       "      <th>count_vec_36</th>\n",
       "      <th>count_vec_37</th>\n",
       "      <th>count_vec_38</th>\n",
       "      <th>count_vec_39</th>\n",
       "      <th>count_vec_40</th>\n",
       "      <th>count_vec_41</th>\n",
       "      <th>count_vec_42</th>\n",
       "      <th>count_vec_43</th>\n",
       "      <th>count_vec_44</th>\n",
       "      <th>count_vec_45</th>\n",
       "      <th>count_vec_46</th>\n",
       "      <th>count_vec_47</th>\n",
       "      <th>count_vec_48</th>\n",
       "      <th>count_vec_49</th>\n",
       "      <th>top_triples_i_should_do_what</th>\n",
       "      <th>top_triples_retailer_canceled_order</th>\n",
       "      <th>top_triples_someone_does_not_plan_to_follow_through_with_the_contract</th>\n",
       "      <th>top_triples_someone_will_file_claims_case</th>\n",
       "      <th>top_triples_lawyer_had_chance</th>\n",
       "      <th>top_triples_that_did_consultation</th>\n",
       "      <th>top_triples_i_sent_retainer_agreement</th>\n",
       "      <th>top_triples_realities_do_not_reflect_commitments</th>\n",
       "      <th>top_triples_that_could_change_lives</th>\n",
       "      <th>top_triples_boss_pulled_girlfriend</th>\n",
       "      <th>liability</th>\n",
       "      <th>insurance</th>\n",
       "      <th>should_do</th>\n",
       "      <th>umap_0</th>\n",
       "      <th>umap_1</th>\n",
       "      <th>umap_2</th>\n",
       "      <th>umap_3</th>\n",
       "      <th>umap_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524</td>\n",
       "      <td>1589605242</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n",
       "      <td>gkoyov</td>\n",
       "      <td>Signed a 2 month sublease with someone in earl...</td>\n",
       "      <td>Sublease Situation</td>\n",
       "      <td>housing</td>\n",
       "      <td>7</td>\n",
       "      <td>[-0.017685881931148586, -0.007538027247924855,...</td>\n",
       "      <td>154</td>\n",
       "      <td>\"Legal implications of breaking a sublease agr...</td>\n",
       "      <td>KS</td>\n",
       "      <td>0</td>\n",
       "      <td>Legal Topics in Rental Properties</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803133</td>\n",
       "      <td>0.297535</td>\n",
       "      <td>0.356883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372940</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.023789</td>\n",
       "      <td>7.921834</td>\n",
       "      <td>2.969897</td>\n",
       "      <td>8.038216</td>\n",
       "      <td>4.074813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6114</td>\n",
       "      <td>1423295631</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n",
       "      <td>2v2pbi</td>\n",
       "      <td>Hey guys, long time (non-lawyer) lurker of the...</td>\n",
       "      <td>[US-TN] Just retained an attorney, what do I n...</td>\n",
       "      <td>family</td>\n",
       "      <td>6</td>\n",
       "      <td>[-0.0018092953279340005, 0.021173241331556625,...</td>\n",
       "      <td>986</td>\n",
       "      <td>\"Key considerations for hiring a family law at...</td>\n",
       "      <td>ID</td>\n",
       "      <td>7</td>\n",
       "      <td>Child Custody and Related Issues</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316155</td>\n",
       "      <td>0.427604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104528</td>\n",
       "      <td>0.182365</td>\n",
       "      <td>0.115107</td>\n",
       "      <td>0.100197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511492</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106901</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140596</td>\n",
       "      <td>0.251371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.225992</td>\n",
       "      <td>7.186953</td>\n",
       "      <td>3.447510</td>\n",
       "      <td>9.862123</td>\n",
       "      <td>4.102845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>564</td>\n",
       "      <td>1590688682</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n",
       "      <td>gsb52f</td>\n",
       "      <td>My boyfriend has a 3 year old daughter current...</td>\n",
       "      <td>Moving A Child Out of State (Oklahoma)</td>\n",
       "      <td>family</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.0077508327692643215, 0.016073830068795918, ...</td>\n",
       "      <td>227</td>\n",
       "      <td>\"Seeking advice on relocating with a child ami...</td>\n",
       "      <td>MT</td>\n",
       "      <td>7</td>\n",
       "      <td>Child Custody and Related Issues</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.268185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.414029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.366739</td>\n",
       "      <td>7.134408</td>\n",
       "      <td>3.828840</td>\n",
       "      <td>9.549561</td>\n",
       "      <td>4.231846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  created_utc                                          full_link  \\\n",
       "0    524   1589605242  https://www.reddit.com/r/legaladvice/comments/...   \n",
       "1   6114   1423295631  https://www.reddit.com/r/legaladvice/comments/...   \n",
       "2    564   1590688682  https://www.reddit.com/r/legaladvice/comments/...   \n",
       "\n",
       "       id                                               body  \\\n",
       "0  gkoyov  Signed a 2 month sublease with someone in earl...   \n",
       "1  2v2pbi  Hey guys, long time (non-lawyer) lurker of the...   \n",
       "2  gsb52f  My boyfriend has a 3 year old daughter current...   \n",
       "\n",
       "                                               title text_label  flair_label  \\\n",
       "0                                 Sublease Situation    housing            7   \n",
       "1  [US-TN] Just retained an attorney, what do I n...     family            6   \n",
       "2             Moving A Child Out of State (Oklahoma)     family            6   \n",
       "\n",
       "                                          embeddings  token_count  \\\n",
       "0  [-0.017685881931148586, -0.007538027247924855,...          154   \n",
       "1  [-0.0018092953279340005, 0.021173241331556625,...          986   \n",
       "2  [0.0077508327692643215, 0.016073830068795918, ...          227   \n",
       "\n",
       "                                           llm_title State  kmeans_label  \\\n",
       "0  \"Legal implications of breaking a sublease agr...    KS             0   \n",
       "1  \"Key considerations for hiring a family law at...    ID             7   \n",
       "2  \"Seeking advice on relocating with a child ami...    MT             7   \n",
       "\n",
       "                         topic_title multiple_topics  topic_0  topic_1  \\\n",
       "0  Legal Topics in Rental Properties             [0]        1        0   \n",
       "1   Child Custody and Related Issues          [0, 2]        1        0   \n",
       "2   Child Custody and Related Issues          [0, 2]        1        0   \n",
       "\n",
       "   topic_2  count_vec_0  count_vec_1  count_vec_2  count_vec_3  count_vec_4  \\\n",
       "0        0     0.000000          0.0     0.000000     0.000000          0.0   \n",
       "1        1     0.000000          0.0     0.316155     0.427604          0.0   \n",
       "2        1     0.268185          0.0     0.000000     0.000000          0.0   \n",
       "\n",
       "   count_vec_5  count_vec_6  count_vec_7  count_vec_8  count_vec_9  \\\n",
       "0     0.000000     0.000000     0.000000     0.000000          0.0   \n",
       "1     0.104528     0.182365     0.115107     0.100197          0.0   \n",
       "2     0.000000     0.000000     0.000000     0.000000          0.0   \n",
       "\n",
       "   count_vec_10  count_vec_11  count_vec_12  count_vec_13  count_vec_14  \\\n",
       "0           0.0           0.0      0.000000           0.0      0.000000   \n",
       "1           0.0           0.0      0.418111           0.0      0.000000   \n",
       "2           0.0           0.0      0.000000           0.0      0.290271   \n",
       "\n",
       "   count_vec_15  count_vec_16  count_vec_17  count_vec_18  count_vec_19  \\\n",
       "0      0.803133      0.297535      0.356883      0.000000           0.0   \n",
       "1      0.000000      0.000000      0.511492      0.102298           0.0   \n",
       "2      0.000000      0.280948      0.000000      0.000000           0.0   \n",
       "\n",
       "   count_vec_20  count_vec_21  count_vec_22  count_vec_23  count_vec_24  \\\n",
       "0           0.0           0.0      0.000000           0.0           0.0   \n",
       "1           0.0           0.0      0.000000           0.0           0.0   \n",
       "2           0.0           0.0      0.336988           0.0           0.0   \n",
       "\n",
       "   count_vec_25  count_vec_26  count_vec_27  count_vec_28  count_vec_29  \\\n",
       "0           0.0      0.000000           0.0           0.0           0.0   \n",
       "1           0.0      0.192644           0.0           0.0           0.0   \n",
       "2           0.0      0.317301           0.0           0.0           0.0   \n",
       "\n",
       "   count_vec_30  count_vec_31  count_vec_32  count_vec_33  count_vec_34  \\\n",
       "0      0.372940       0.00000           0.0           0.0           0.0   \n",
       "1      0.106901       0.00000           0.0           0.0           0.0   \n",
       "2      0.000000       0.37918           0.0           0.0           0.0   \n",
       "\n",
       "   count_vec_35  count_vec_36  count_vec_37  count_vec_38  count_vec_39  \\\n",
       "0           0.0      0.000000           0.0           0.0      0.000000   \n",
       "1           0.0      0.243622           0.0           0.0      0.000000   \n",
       "2           0.0      0.000000           0.0           0.0      0.414029   \n",
       "\n",
       "   count_vec_40  count_vec_41  count_vec_42  count_vec_43  count_vec_44  \\\n",
       "0           0.0           0.0           0.0           0.0      0.000000   \n",
       "1           0.0           0.0           0.0           0.0      0.140596   \n",
       "2           0.0           0.0           0.0           0.0      0.000000   \n",
       "\n",
       "   count_vec_45  count_vec_46  count_vec_47  count_vec_48  count_vec_49  \\\n",
       "0      0.000000      0.000000           0.0           0.0      0.000000   \n",
       "1      0.251371      0.000000           0.0           0.0      0.109438   \n",
       "2      0.000000      0.485231           0.0           0.0      0.000000   \n",
       "\n",
       "   top_triples_i_should_do_what  top_triples_retailer_canceled_order  \\\n",
       "0                             0                                    0   \n",
       "1                             0                                    0   \n",
       "2                             0                                    0   \n",
       "\n",
       "   top_triples_someone_does_not_plan_to_follow_through_with_the_contract  \\\n",
       "0                                                  1                       \n",
       "1                                                  0                       \n",
       "2                                                  0                       \n",
       "\n",
       "   top_triples_someone_will_file_claims_case  top_triples_lawyer_had_chance  \\\n",
       "0                                          1                              0   \n",
       "1                                          0                              1   \n",
       "2                                          0                              0   \n",
       "\n",
       "   top_triples_that_did_consultation  top_triples_i_sent_retainer_agreement  \\\n",
       "0                                  0                                      0   \n",
       "1                                  1                                      1   \n",
       "2                                  0                                      0   \n",
       "\n",
       "   top_triples_realities_do_not_reflect_commitments  \\\n",
       "0                                                 0   \n",
       "1                                                 1   \n",
       "2                                                 0   \n",
       "\n",
       "   top_triples_that_could_change_lives  top_triples_boss_pulled_girlfriend  \\\n",
       "0                                    0                                   0   \n",
       "1                                    0                                   0   \n",
       "2                                    1                                   0   \n",
       "\n",
       "   liability  insurance  should_do     umap_0    umap_1    umap_2    umap_3  \\\n",
       "0          0          0          0  12.023789  7.921834  2.969897  8.038216   \n",
       "1          0          0          0   9.225992  7.186953  3.447510  9.862123   \n",
       "2          0          0          0   9.366739  7.134408  3.828840  9.549561   \n",
       "\n",
       "     umap_4  \n",
       "0  4.074813  \n",
       "1  4.102845  \n",
       "2  4.231846  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test_res.iloc[:, 14:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res['target'] = np.where(test_res['text_label']==\"employment\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import ProbeFeatureSelection\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = ProbeFeatureSelection(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    variables=None,\n",
    "    scoring=\"precision\",\n",
    "    n_probes=1,\n",
    "    distribution=\"normal\",\n",
    "    cv=5,\n",
    "    random_state=150,\n",
    "    confirm_variables=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ProbeFeatureSelection(estimator=RandomForestClassifier(), random_state=150,\n",
       "                      scoring=&#x27;precision&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ProbeFeatureSelection</label><div class=\"sk-toggleable__content\"><pre>ProbeFeatureSelection(estimator=RandomForestClassifier(), random_state=150,\n",
       "                      scoring=&#x27;precision&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ProbeFeatureSelection(estimator=RandomForestClassifier(), random_state=150,\n",
       "                      scoring='precision')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.fit(df_test, test_res['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_0        0.008615\n",
       "topic_1        0.011466\n",
       "topic_2        0.005186\n",
       "count_vec_0    0.014294\n",
       "count_vec_1    0.004861\n",
       "count_vec_2    0.013218\n",
       "count_vec_3    0.016269\n",
       "count_vec_4    0.007265\n",
       "count_vec_5    0.014331\n",
       "count_vec_6    0.007042\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.feature_importances_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_0',\n",
       " 'topic_1',\n",
       " 'topic_2',\n",
       " 'count_vec_0',\n",
       " 'count_vec_1',\n",
       " 'count_vec_2',\n",
       " 'count_vec_3',\n",
       " 'count_vec_4',\n",
       " 'count_vec_5',\n",
       " 'count_vec_6',\n",
       " 'count_vec_7',\n",
       " 'count_vec_9',\n",
       " 'count_vec_10',\n",
       " 'count_vec_11',\n",
       " 'count_vec_12',\n",
       " 'count_vec_13',\n",
       " 'count_vec_14',\n",
       " 'count_vec_15',\n",
       " 'count_vec_16',\n",
       " 'count_vec_17',\n",
       " 'count_vec_18',\n",
       " 'count_vec_19',\n",
       " 'count_vec_20',\n",
       " 'count_vec_21',\n",
       " 'count_vec_22',\n",
       " 'count_vec_23',\n",
       " 'count_vec_24',\n",
       " 'count_vec_25',\n",
       " 'count_vec_26',\n",
       " 'count_vec_27',\n",
       " 'count_vec_28',\n",
       " 'count_vec_29',\n",
       " 'count_vec_30',\n",
       " 'count_vec_31',\n",
       " 'count_vec_32',\n",
       " 'count_vec_33',\n",
       " 'count_vec_34',\n",
       " 'count_vec_35',\n",
       " 'count_vec_36',\n",
       " 'count_vec_37',\n",
       " 'count_vec_38',\n",
       " 'count_vec_39',\n",
       " 'count_vec_40',\n",
       " 'count_vec_41',\n",
       " 'count_vec_42',\n",
       " 'count_vec_43',\n",
       " 'count_vec_44',\n",
       " 'count_vec_45',\n",
       " 'count_vec_46',\n",
       " 'count_vec_47',\n",
       " 'count_vec_48',\n",
       " 'count_vec_49',\n",
       " 'top_triples_i_should_do_what',\n",
       " 'top_triples_retailer_canceled_order',\n",
       " 'top_triples_someone_does_not_plan_to_follow_through_with_the_contract',\n",
       " 'top_triples_someone_will_file_claims_case',\n",
       " 'top_triples_lawyer_had_chance',\n",
       " 'top_triples_that_did_consultation',\n",
       " 'top_triples_i_sent_retainer_agreement',\n",
       " 'top_triples_realities_do_not_reflect_commitments',\n",
       " 'top_triples_that_could_change_lives',\n",
       " 'top_triples_boss_pulled_girlfriend',\n",
       " 'liability',\n",
       " 'insurance',\n",
       " 'should_do',\n",
       " 'umap_0',\n",
       " 'umap_2',\n",
       " 'umap_3']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.features_to_drop_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
