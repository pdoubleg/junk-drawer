{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105a2123",
   "metadata": {},
   "source": [
    "# Comparing Retrieval Augmented Generation (RAG) Methods with Graphs\n",
    "\n",
    "In this notebook we will explore and compare several RAG methods with a focus on knowledge graphs.\n",
    "\n",
    "## Background on RAG:\n",
    "\n",
    "Below digrams show how RAG works:\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In VectorDB based RAG, we create embeddings of each node (chunk), and find TopK related ones towards a given question during the query. In the above diagram, nodes `3` and `96` were fetched as the TopK related nodes and used to answer the user query. \n",
    "\n",
    "## Background Graph RAG\n",
    "\n",
    "In Graph RAG, we will extract relationships between representing concise facts from each node. It would look something like this:\n",
    "\n",
    "```\n",
    "Node Split and Embedding\n",
    "\n",
    "┌────┬────┬────┬────┐\n",
    "│ 1  │ 2  │ 3  │ 4  │\n",
    "├────┴────┴────┴────┤\n",
    "│  Docs/Knowledge   │\n",
    "│        ...        │\n",
    "├────┬────┬────┬────┤\n",
    "│ 95 │ 96 │    │    │\n",
    "└────┴────┴────┴────┘\n",
    "```\n",
    "\n",
    "If we zoomed in:\n",
    "\n",
    "```\n",
    "       Node Split and Embedding, with Knowledge Graph being extracted\n",
    "\n",
    "┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
    "│ .─.       .─.    │  .─.       .─.   │            .─.   │  .─.       .─.   │\n",
    "│( x )─────▶ y )   │ ( x )─────▶ a )  │           ( j )  │ ( m )◀────( x )  │\n",
    "│ `▲'       `─'    │  `─'       `─'   │            `─'   │  `─'       `─'   │\n",
    "│  │     1         │        2         │        3    │    │        4         │\n",
    "│ .─.              │                  │            .▼.   │                  │\n",
    "│( z )─────────────┼──────────────────┼──────────▶( i )─┐│                  │\n",
    "│ `◀────┐          │                  │            `─'  ││                  │\n",
    "├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤\n",
    "│       │                      Docs/Knowledge           │                   │\n",
    "│       │                            ...                │                   │\n",
    "│       │                                               │                   │\n",
    "├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤\n",
    "│  .─.  └──────.   │  .─.             │                 ││  .─.             │\n",
    "│ ( x ◀─────( b )  │ ( x )            │                 └┼▶( n )            │\n",
    "│  `─'       `─'   │  `─'             │                  │  `─'             │\n",
    "│        95   │    │   │    96        │                  │   │    98        │\n",
    "│            .▼.   │  .▼.             │                  │   ▼              │\n",
    "│           ( c )  │ ( d )            │                  │  .─.             │\n",
    "│            `─'   │  `─'             │                  │ ( x )            │\n",
    "└──────────────────┴──────────────────┴──────────────────┴──`─'─────────────┘\n",
    "```\n",
    "\n",
    "In theory, knowledge graphs should help balance granularity and density. Optionally, multi-hop of `x -> y`, `i -> j -> z -> x` etc... across many more nodes (chunks) than TopK search allows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75004d1",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061a39e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import (\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5363496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "# import os\n",
    "# import json\n",
    "# import openai\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from llama_index.llms import AzureOpenAI\n",
    "# from llama_index import LangchainEmbedding\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    KnowledgeGraphIndex,\n",
    ")\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores.simple import SimpleGraphStore\n",
    "\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = \"https://<foo-bar>.openai.azure.com\"\n",
    "# openai.api_version = \"2022-12-01\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# llm = AzureOpenAI(\n",
    "#     engine=\"<foo-bar-deployment>\",\n",
    "#     temperature=0,\n",
    "#     openai_api_version=openai.api_version,\n",
    "#     model_kwargs={\n",
    "#         \"api_key\": openai.api_key,\n",
    "#         \"api_base\": openai.api_base,\n",
    "#         \"api_type\": openai.api_type,\n",
    "#         \"api_version\": openai.api_version,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "# embedding_llm = LangchainEmbedding(\n",
    "#     OpenAIEmbeddings(\n",
    "#         model=\"text-embedding-ada-002\",\n",
    "#         deployment=\"<foo-bar-deployment>\",\n",
    "#         openai_api_key=openai.api_key,\n",
    "#         openai_api_base=openai.api_base,\n",
    "#         openai_api_type=openai.api_type,\n",
    "#         openai_api_version=openai.api_version,\n",
    "#     ),\n",
    "#     embed_batch_size=1,\n",
    "# )\n",
    "\n",
    "# embedding_llm = LangchainEmbedding(\n",
    "#     OpenAIEmbeddings())\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embedding_llm,\n",
    "# )\n",
    "\n",
    "# set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17442e",
   "metadata": {},
   "source": [
    "## 1.2. Prepare  Graph Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cf0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = SimpleGraphStore()\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5613a",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "\n",
    "The Knowledge Graph is created with the `KnowledgeGraphIndex` from Llama Index. This extracts \"Triplets\" that will be persisted in `SimpleGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb7083",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess Data\n",
    "\n",
    "We will download and preprecess data from:\n",
    "    https://www.iii.org/sites/default/files/docs/pdf/HO3_sample.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e50f03d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m SimpleDirectoryReader\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclean_sample_ho3\u001b[39;00m \u001b[39mimport\u001b[39;00m clean_sample_ho3_pages\n\u001b[0;32m      4\u001b[0m documents \u001b[39m=\u001b[39m SimpleDirectoryReader(input_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m../data/HO3_sample.pdf\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(documents):\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from ..src_index.clean_sample_ho3 import clean_sample_ho3_pages\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['../data/HO3_sample.pdf']).load_data()\n",
    "\n",
    "for i, _ in enumerate(documents):\n",
    "    documents[i].text = clean_sample_ho3_pages(documents[i].text)\n",
    "    documents[i].text = documents[0].text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35757f5",
   "metadata": {},
   "source": [
    "* PDF page count for the HO3 Homeowner's policy document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf402428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948d288",
   "metadata": {},
   "source": [
    "* Prompt to build the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266bb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.base import Prompt\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "\n",
    "\n",
    "KG_TRIPLET_EXTRACT_TMPL = (\n",
    "    \"Some text is provided below. Given the text, extract up to \"\n",
    "    \"{max_knowledge_triplets} \"\n",
    "    \"knowledge triplets in the form of (subject, predicate, object). Avoid stopwords, page_label, and numbers.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Example:\"\n",
    "    \"Text: Alice is Bob's mother.\"\n",
    "    \"Triplets:\\n(Alice, is mother of, Bob)\\n\"\n",
    "    \"Text: Philz is a coffee shop founded in Berkeley in 1982.\\n\"\n",
    "    \"Triplets:\\n\"\n",
    "    \"(Philz, is, coffee shop)\\n\"\n",
    "    \"(Philz, founded in, Berkeley)\\n\"\n",
    "    \"(Philz, founded in, 1982)\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Text: {text}\\n\"\n",
    "    \"Triplets:\\n\"\n",
    ")\n",
    "KG_TRIPLET_EXTRACT_PROMPT = Prompt(\n",
    "    KG_TRIPLET_EXTRACT_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21c03b",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to SimpleGraphStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f3668",
   "metadata": {},
   "source": [
    "This call will take some time, it'll extract entities and relationships and store them in SimpleGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05437d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents into nodes: 100%|██████████| 22/22 [00:00<00:00, 48.24it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 34.88it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 34.21it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 17.15it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 30.66it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 39.71it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  5.89it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:00<00:00, 50.60it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 44.00it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 28.19it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 73.03it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 34.34it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 32.39it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 91.84it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 56.30it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 14.85it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 40.46it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 53.92it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 32.61it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 94.89it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 35.12it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  5.05it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 95.61it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 34.10it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 39.97it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 24.35it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 26.28it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 26.04it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 49.51it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 43.36it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 33.42it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 41.96it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 46.61it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 26.62it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00, 29.54it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 47.20it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 24.96it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00, 45.11it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 46.03it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 37.46it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 56.16it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 38.75it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 21.51it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 37.40it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 41.69it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 22.26it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00, 27.49it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 45.49it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 23.94it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 45.68it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 48.58it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 20.11it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 51.03it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 44.94it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 25.90it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 36.74it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 26.08it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 21.99it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 53.97it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 39.42it/s]\n",
      "Processing nodes:  89%|████████▉ | 59/66 [03:16<00:28,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message='Request failed due to server shutdown' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='Request failed due to server shutdown' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "error_code=None error_message='Request failed due to server shutdown' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "WARNING:llama_index.llms.openai_utils:Retrying llama_index.llms.openai_utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Request failed due to server shutdown {\n",
      "  \"error\": {\n",
      "    \"message\": \"Request failed due to server shutdown\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 24 Jul 2023 05:33:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-003', 'openai-organization': 'user-dpezcjixjzcaiwozxi0vtmcs', 'openai-processing-ms': '1260', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '246078', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '941ms', 'x-request-id': 'd090e4a5f0f10488cf683c1b9fa195de', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7eb9c9ceacf6aa91-DFW', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying llama_index.llms.openai_utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Request failed due to server shutdown {\n",
      "  \"error\": {\n",
      "    \"message\": \"Request failed due to server shutdown\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 24 Jul 2023 05:33:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-003', 'openai-organization': 'user-dpezcjixjzcaiwozxi0vtmcs', 'openai-processing-ms': '1260', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '246078', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '941ms', 'x-request-id': 'd090e4a5f0f10488cf683c1b9fa195de', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7eb9c9ceacf6aa91-DFW', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying llama_index.llms.openai_utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Request failed due to server shutdown {\n",
      "  \"error\": {\n",
      "    \"message\": \"Request failed due to server shutdown\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 24 Jul 2023 05:33:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-003', 'openai-organization': 'user-dpezcjixjzcaiwozxi0vtmcs', 'openai-processing-ms': '1260', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '246078', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '941ms', 'x-request-id': 'd090e4a5f0f10488cf683c1b9fa195de', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7eb9c9ceacf6aa91-DFW', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 40.50it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 42.11it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 54.43it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 36.79it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:00<00:00, 23.12it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:00<00:00, 40.23it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 29.46it/s]\n",
      "Processing nodes: 100%|██████████| 66/66 [03:44<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import OpenAIEmbedding, ServiceContext, KnowledgeGraphIndex\n",
    "\n",
    "embedding_llm = OpenAIEmbedding()\n",
    "\n",
    "llm=OpenAI(temperature=0, \n",
    "           model_name=\"gpt-3.5-turbo\",\n",
    "           )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embedding_llm,\n",
    "                                               chunk_size=512,\n",
    "                                               chunk_overlap=100\n",
    "                                               )\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    max_triplets_per_chunk=10,\n",
    "    storage_context=storage_context,\n",
    "    include_embeddings=True,\n",
    "    kg_triple_extract_template=KG_TRIPLET_EXTRACT_PROMPT,\n",
    "    show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3efdc4",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG\n",
    "\n",
    "To compare Graph-based query with vector similarity we'll also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, the same data source will be split into chunks and converted to embeddings. During query time, the top-k related embeddings will be vector-searched with the embedding of the question.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "The next line of code does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24a204",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kg_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# persist KG Index\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m kg_index\u001b[39m.\u001b[39mset_index_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkg_ho3_policy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m kg_index\u001b[39m.\u001b[39mstorage_context\u001b[39m.\u001b[39mpersist(persist_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./ho3_storage_kg_graph\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# persist Vector Index\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kg_index' is not defined"
     ]
    }
   ],
   "source": [
    "# persist KG Index\n",
    "kg_index.set_index_id = \"kg_ho3_policy\"\n",
    "kg_index.storage_context.persist(persist_dir='./ho3_storage_kg_graph')\n",
    "\n",
    "# persist Vector Index\n",
    "vector_index.set_index_id = \"vector_ho3_policy\"\n",
    "vector_index.storage_context.persist(persist_dir='./ho3_storage_kg_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c2aee",
   "metadata": {},
   "source": [
    "## 4. Persist and Load index objects from disk\n",
    "\n",
    "Both the `KnowledgeGraphIndex` and `VectorStoreIndex` will be created only once. Afterwards, we will persist and re-use them. Adding IDs is optional and only needed when multiple index objects exist in the same directory.\n",
    "\n",
    "#### Persist\n",
    "\n",
    "```python\n",
    "# persist KG Index\n",
    "kg_index.set_index_id = \"kg_ho3_policy\"\n",
    "kg_index.storage_context.persist(persist_dir='./storage_graph')\n",
    "\n",
    "# persist Vector Index\n",
    "vector_index.set_index_id = \"vector_ho3_policy\"\n",
    "vector_index.storage_context.persist(persist_dir='./storage_vector')\n",
    "\n",
    "```\n",
    "#### Restore\n",
    "\n",
    "Restore the index from disk like:\n",
    "\n",
    "```python\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "# Set ServiceContext if it hasn't already been done\n",
    "embedding_llm = OpenAIEmbedding()\n",
    "llm=OpenAI(temperature=0, \n",
    "           model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embedding_llm,\n",
    "                                               chunk_size=512,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all indices.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import (\n",
    "    OpenAIEmbedding, \n",
    "    ServiceContext, \n",
    "    load_index_from_storage, \n",
    "    set_global_service_context\n",
    ")\n",
    "\n",
    "\n",
    "embedding_llm = OpenAIEmbedding()\n",
    "\n",
    "llm=OpenAI(temperature=0, \n",
    "           model_name=\"gpt-3.5-turbo\",\n",
    "           )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embedding_llm,\n",
    "                                               chunk_size=512,\n",
    "                                               )\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "storage_context_kg = StorageContext.from_defaults(graph_store=graph_store, persist_dir=\"./ho3_storage_kg_graph\")\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context_kg,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True, \n",
    "    )\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./ho3_storage_kg_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    storage_context=storage_context_vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb936",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches\n",
    "\n",
    "We will do 4 types of query approaches with LLM, KG, VectorDB:\n",
    "\n",
    "| QueryEngine | Knowledge Graph Only                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Graph Only** based on triplets<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5524b2e",
   "metadata": {},
   "source": [
    "### 5.1 Knowledge Graph Only\n",
    "\n",
    "This approach strictly follows the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_query_engine = kg_index.as_query_engine(\n",
    "    # Uses the raw triplets instead of adding the text from the corresponding nodes\n",
    "    include_text=True,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e8a9b",
   "metadata": {},
   "source": [
    "### 5.1.1 Knowledge Graph Hybrid - Search \n",
    "\n",
    "Here, we add flexibility from embedding representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05673f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query using top 3 triplets plus keywords (duplicate triplets are removed)\n",
    "hybrid_query_engine = kg_index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c590de2",
   "metadata": {},
   "source": [
    "### 5.2 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me about x, please │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ Below are knowledge about x │             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             │ Please answer based on them │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d72cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06682474",
   "metadata": {},
   "source": [
    "### 5.3 Standard vector query engine\n",
    "\n",
    "Vector RAG to find topK semantic related doc chunks, and use as context for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37713b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93396d7",
   "metadata": {},
   "source": [
    "### 5.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0ee74",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context for the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐┌────┐               │             \n",
    "               │ 3  ││ 96 │ x->y<-z,x->h...                            \n",
    "             │ └────┘└────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9516c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe22b2b",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67a63f",
   "metadata": {},
   "source": [
    "And the query engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4976682",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541608be",
   "metadata": {},
   "source": [
    "## 6. Query with all the Engines\n",
    "* We will ask a purposefully vague and open-ended query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384be7b",
   "metadata": {},
   "source": [
    "### 6.1 Keyword Knowledge Graph Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acaa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about the limit of liability.\n",
      "> Starting query: Tell me about the limit of liability.\n",
      "> Starting query: Tell me about the limit of liability.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The limit of liability is the maximum amount of money that a person or company is legally responsible for paying in the event of a lawsuit or other legal action. It is typically specified in a contract or insurance policy and is intended to protect the party from excessive financial losses.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_kg = kg_query_engine.query(\"Tell me about the limit of liability.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_kg}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bafef8",
   "metadata": {},
   "source": [
    "### 6.2 Graph Keyword Embeddings Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about the limit of liability.\n",
      "> Starting query: Tell me about the limit of liability.\n",
      "> Starting query: Tell me about the limit of liability.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Querying with idx: a7221853-4e5b-402f-ba1a-1384b2ce7496: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: a7221853-4e5b-402f-ba1a-1384b2ce7496: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: a7221853-4e5b-402f-ba1a-1384b2ce7496: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Querying with idx: f880f431-40a7-4118-aaaf-11430ce570c4: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: f880f431-40a7-4118-aaaf-11430ce570c4: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: f880f431-40a7-4118-aaaf-11430ce570c4: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Querying with idx: f2e6c4cb-a659-4a47-942e-24a612506008: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: f2e6c4cb-a659-4a47-942e-24a612506008: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: f2e6c4cb-a659-4a47-942e-24a612506008: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Querying with idx: 07b93270-f047-4e7a-8473-43d04e8a24e3: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: 07b93270-f047-4e7a-8473-43d04e8a24e3: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: 07b93270-f047-4e7a-8473-43d04e8a24e3: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Querying with idx: a83519ab-a3f3-4eab-96cf-3587c9c4e0b7: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: a83519ab-a3f3-4eab-96cf-3587c9c4e0b7: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: a83519ab-a3f3-4eab-96cf-3587c9c4e0b7: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Querying with idx: 07d73831-1bcf-44dc-ac0e-873111d11deb: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: 07d73831-1bcf-44dc-ac0e-873111d11deb: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "> Querying with idx: 07d73831-1bcf-44dc-ac0e-873111d11deb: homeowners\n",
      "ho 00 03 10 00\n",
      "ho 00 03 10 00 copyright, insurance services office...\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "('Motor vehicle liability', 'covers', 'bodily injury')\n",
      "('Watercraft liability', 'covers', 'bodily injury')\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "('Motor vehicle liability', 'covers', 'bodily injury')\n",
      "('Watercraft liability', 'covers', 'bodily injury')\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "('Motor vehicle liability', 'covers', 'bodily injury')\n",
      "('Watercraft liability', 'covers', 'bodily injury')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The limit of liability for motor vehicle and watercraft liability is the amount of coverage provided for bodily injury. This coverage is subject to the provisions of the policy and may vary depending on the type of vehicle or craft.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_hybrid = hybrid_query_engine.query(\"Tell me about the limit of liability.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_hybrid}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c800",
   "metadata": {},
   "source": [
    "### 6.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa71a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The limit of liability in this policy is the maximum amount of coverage that is provided for any claims arising from the ownership, maintenance, occupancy, operation, use, loading or unloading of an aircraft, hovercraft, motor vehicle, or watercraft by an insured. This limit of liability is subject to the provisions outlined in the policy.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about the limit of liability\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e1b07",
   "metadata": {},
   "source": [
    "### 6.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about the limit of liability.\n",
      "> Starting query: Tell me about the limit of liability.\n",
      "> Starting query: Tell me about the limit of liability.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "> Query keywords: ['Limit', 'Responsibility', 'Liability']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The limit of liability is defined in the policy as the maximum amount of money that the company providing the insurance will pay out in the event of a claim. This amount is typically determined by the type of coverage purchased and the amount of the premium paid. The policy may also specify certain exclusions or limitations on the coverage, such as the types of losses that are not covered or the maximum amount of liability for certain types of losses.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about the limit of liability.\")\n",
    "\n",
    "Markdown(f\"<b>{response_graph_vector_rag}</b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f82da8",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697e4bc",
   "metadata": {},
   "source": [
    "### 7.1 Overall Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a new model for QA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "qa_llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717df1d",
   "metadata": {},
   "source": [
    "* Asking GPT-3.5-Turbo to compare query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c776532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Knowledge Facts | GraphQuery | GraphHybrid | Vector | Graph+Vector |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| Definition | The maximum amount of money that a person or company is legally responsible for paying in the event of a lawsuit or other legal action. | The amount of coverage provided for bodily injury. | The maximum amount of coverage that is provided for any claims arising from the ownership, maintenance, occupancy, operation, use, loading or unloading of an aircraft, hovercraft, motor vehicle, or watercraft by an insured. | The maximum amount of money that the company providing the insurance will pay out in the event of a claim. |\n",
       "| Specified in | Contract or insurance policy | Policy | Policy | Policy |\n",
       "| Intended to | Protect the party from excessive financial losses | - | - | - |\n",
       "| Coverage varies | - | Depending on the type of vehicle or craft | - | Depending on the type of coverage purchased and the amount of the premium paid |\n",
       "| Exclusions/Limitations | - | - | Outlined in the policy | Types of losses that are not covered or the maximum amount of liability for certain types of losses |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Markdown(\n",
    "    qa_llm(f\"\"\"\n",
    "Compare the QA results on \"Explain Limit of Liability\", list the knowledge facts between them to help evalute them. Output in markdown table.\n",
    "\n",
    "Result GraphQuery: {response_kg}\n",
    "---\n",
    "Result GraphHybrid: {response_hybrid}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652985c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Result from Graph | Result from Graph_hybrid |\n",
       "| --- | --- |\n",
       "| The limit of liability is the maximum amount of money that a person or company is legally responsible for paying in the event of a lawsuit or other legal action. It is typically specified in a contract or insurance policy and is intended to protect the party from excessive financial losses. | The limit of liability for motor vehicle and watercraft liability is the amount of coverage provided for bodily injury. This coverage is subject to the provisions of the policy and may vary depending on the type of vehicle or craft. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Markdown(\n",
    "    qa_llm(f\"\"\"\n",
    "Compare the two QA result on \"Explain Limit of Liability\", list the differences between them to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_kg}\n",
    "---\n",
    "Result from Graph_hybrid: {response_hybrid}\n",
    "\n",
    "\"\"\"     )\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4510b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Result | Topk Vecs | Graph+Vec |\n",
       "| --- | --- | --- |\n",
       "| Definition | Maximum amount of coverage provided for any claims arising from the ownership, maintenance, occupancy, operation, use, loading or unloading of an aircraft, hovercraft, motor vehicle, or watercraft by an insured. | Maximum amount of money that the company providing the insurance will pay out in the event of a claim. |\n",
       "| Factors | Subject to the provisions outlined in the policy. | Type of coverage purchased and the amount of the premium paid. The policy may also specify certain exclusions or limitations on the coverage. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Markdown(\n",
    "    qa_llm(f\"\"\"\n",
    "Compare the two QA results on \"Explain Limit of Liability\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Topk Vecs: {response_vector_rag}\n",
    "---\n",
    "Result from Graph+Vec: {response_graph_vector_rag}\n",
    "\n",
    "\"\"\"     )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cee5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and export network graph\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "g = kg_index.get_networkx_graph()\n",
    "net = Network(\n",
    "    notebook=True,\n",
    "    directed=True,\n",
    "    cdn_resources='in_line',\n",
    ")\n",
    "\n",
    "net = Network(\n",
    "    cdn_resources='local',\n",
    "    directed = True,            # directed graph\n",
    "    bgcolor = \"#222222\",          # background color of graph \n",
    "    font_color = \"white\",      # use yellow for node labels\n",
    "    height = \"1000px\",          # height of chart\n",
    "    width = \"100%\",             # fill the entire width    \n",
    "    )\n",
    "\n",
    "net.repulsion(\n",
    "    node_distance=200,\n",
    "    central_gravity=0.2,\n",
    "    spring_length=250,\n",
    "    spring_strength=0.08,\n",
    "    damping=0.1,\n",
    ")\n",
    "net.toggle_physics(True)\n",
    "net.from_nx(g)\n",
    "net.write_html(\"kg_index_ho3.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6c06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
